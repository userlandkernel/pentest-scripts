#!/usr/bin/env python3
import sys
import requests


class RobotsScanner:

    def __init__(self, domainfile = "", resultfile = ""):
        self.domainfile = open(domainfile, "r+")
        print("Initialized. IDLE.")

    def startscan(self):

            for line in self.domainfile:
                with open(line+"-robots.txt") as file:
                    if file:
                        file.close()
                        continue

                url = str("http://"+line + "/robots.txt")
                rsp = requests.get(url)

                if rsp.status_code == 200 or rsp.status_code == 403:
                    with open(line+"-robots.txt") as robots:
                        robots.write(str(rsp.status_code)+"\n"+str(rsp.raw))
                        robots.close()

                elif rsp.status_code == 404:
                    print(url+ " [No Robots]")

                else:
                    print(url+" [Weird status ("+str(rsp.status_code)+")]")

            self.domainfile.close()

def usage():
    print(str(sys.argv[0])+ "[file-with-domains.txt]")

def main():
    argc = len(sys.argv)

    if argc == 0:
        usage()
    else:
        scraper = RobotsScanner(sys.argv[1])
        scraper.startscan()

    print("Shots fired nigga, we done. Gang gang.")
    exit(1337)

main()
